install.packages("shiny")
help("unique")
library(knitr)
library(tidyverse)
library(magrittr)
library(kableExtra)
opts_chunk$set(echo = FALSE,
warning = FALSE,
message = FALSE)
## read the data
ag_data <- read_csv("berries.csv", col_names = TRUE)
## look at number of unique values in each column
ag_data %>% summarize_all(n_distinct) -> aa
## make a list of the columns with only one unique value
bb <- which(aa[1,]==1)
## list the 1-unique valu column names
cn <- colnames(ag_data)[bb]
aa
bb
cn
## remove the 1-unique columns from the dataset
ag_data %<>% select(-all_of(bb))
aa %<>% select(-all_of(bb))
## State name and the State ANSI code are (sort of) redundant
## Just keep the name
ag_data %<>% select(-4)
aa %<>% select(-4)
kable(head(ag_data)) %>% kable_styling(font_size=12)
aa
berry <- unique(ag_data$Commodity)
nberry <- length(berry)
berry
bberry <- ag_data %>% filter((Commodity=="BLUEBERRIES") & (Period=="YEAR"))
bberry %<>% select(-c(Period, Commodity))
#### Does every Data Item begin with "
sum(str_detect(bberry$`Data Item`, "^BLUEBERRIES, ")) == length(bberry$`Data Item`)
# di <- bberry$`Data Item`
# di_m <- str_split(di, ",", simplify=TRUE)
# dim(di_m)
#
# unique(di_m[,1])
# di_m <- di_m[,2:4]
bberry %<>% separate(`Data Item`, c("B","type", "meas", "what"), sep = ",")
bberry %<>% select(-B)
# head(bberry$type, n=20)
# ty <- str_split(bberry$type, " ", simplify=TRUE)
# head(ty, n=20)
bberry %<>% separate(type,c("b1", "type", "b2", "lab1", "lab2"), " ")
bberry %<>% select(-c(b1,b2))
#kable(head(bberry, n=10)) %>% kable_styling(font_size=12)
library(knitr)
library(tidyverse)
library(magrittr)
library(kableExtra)
opts_chunk$set(echo = FALSE,
warning = FALSE,
message = FALSE)
## read the data
ag_data <- read_csv("berries.csv", col_names = TRUE)
## look at number of unique values in each column
ag_data %>% summarize_all(n_distinct) -> aa
## make a list of the columns with only one unique value
bb <- which(aa[1,]==1)
## list the 1-unique valu column names
cn <- colnames(ag_data)[bb]
## remove the 1-unique columns from the dataset
ag_data %<>% select(-all_of(bb))
aa %<>% select(-all_of(bb))
## State name and the State ANSI code are (sort of) redundant
## Just keep the name
ag_data %<>% select(-4)
aa %<>% select(-4)
kable(head(ag_data)) %>% kable_styling(font_size=12)
berry <- unique(ag_data$Commodity)
nberry <- length(berry)
bberry <- ag_data %>% filter((Commodity=="BLUEBERRIES") & (Period=="YEAR"))
bberry %<>% select(-c(Period, Commodity))
bberry
#### Does every Data Item begin with "
#BLUEBERRIES, TAME - PRICE RECEIVED, MEASURED IN $ / LB
sum(str_detect(bberry$`Data Item`, "^BLUEBERRIES, ")) == length(bberry$`Data Item`)
sum(str_detect(bberry$`Data Item`, "^BLUEBERRIES, "))
length(bberry$`Data Item`)
help("str_detect")
help(^)
bberry %<>% separate(`Data Item`, c("B","type", "meas", "what"), sep = ",")
bberry
bberry$meas
bberry$waht
bberry$what
head(bberry$type, n=20)
bberry %<>% select(-B)
bberry %<>% separate(type,c("b1", "type", "b2", "lab1", "lab2"), " ")
bberry
bberry %<>% select(-c(b1,b2))
bberry
kable(head(bberry, n=10)) %>% kable_styling(font_size=12)
ggcounty.us()
library(ggplot2)
ggcounty.us()
knitr::opts_chunk$set(echo = TRUE)
view(rain)
rain
library(leaflet)
library(maps)
library(RColorBrewer)
mapStates <- map("state", fill = TRUE, plot = FALSE)
leaflet(data = mapStates) %>% addPolygons(color = "black",weight = 0.5,fillColor ="white",fillOpacity = 1)
rain$state=fips_info(rain$fips)$full
library(maps)
library(tidyverse)
library(tidyverse)
library(drat)
library(hurricaneexposuredata)
library(hurricaneexposure)
mapStates <- map("state", fill = TRUE, plot = FALSE)
rain$state=fips_info(rain$fips)$full
fips_info
library(sp)
rain$state=fips_info(rain$fips)$full
??fips_info
library(usmap)
rain$state=fips_info(rain$fips)$full
rain$state=fips_info(rain$fips)$full
rain$county=fips_info(rain$fips)$county
rain$county=fips_info(rain$fips)$county
rain
mapStates
View(rain)
county <- map_data("county")
data(county.fips)
county.fips %>%
as_tibble %>%
separate(polyname, c("region", "subregion"), "," ) -> dfips
map_data("county") %>%
left_join(dfips) ->
dall
county.fips
dfips
dall
leaflet(data = county) %>% addTiles() %>% addPolygons()
mapStates$x
mapStates$names
mapStates <- map("county", fill = TRUE, plot = FALSE)
mapStates <- map("county", fill = TRUE, plot = FALSE)
mapStates <- map("state", fill = TRUE, plot = FALSE)
mapStates <- map("state", fill = TRUE, plot = FALSE)
mapStates <- map("state", fill = TRUE, plot = FALSE)
library(maps)
library(usmap)
mapStates <- map("state", fill = TRUE, plot = FALSE)
mapStates <- map("state", fill = TRUE, plot = FALSE)
map("state", fill = TRUE, plot = FALSE)
mapStates <- map("state", fill = TRUE, plot = FALSE)
mapStates$names
help(map)
mapStates <- map("usa", fill = TRUE, plot = FALSE)
map("state")
library(maps)
map("state")
mapStates <- map("usa", fill = TRUE, plot = FALSE)
mapStates = map("state", fill = TRUE, plot = FALSE)
rain$state=fips_info(rain$fips)$full
rain$state=fips_info(rain$fips)$full
rain$county=fips_info(rain$fips)$county
rain
rain
mapStates = map("state", fill = TRUE, plot = FALSE)
mapStates = map("state")
map("usa")
library(leaflet)
library(maps)
library(RColorBrewer)
library(usmap)
library(tidyverse)
library(drat)
library(hurricaneexposuredata)
library(hurricaneexposure)
mapStates = map("state")
mapStates = ma_data("state")
mapStates = map_data("state")
leaflet(data = mapStates) %>% addPolygons(color = "black",weight = 0.5,fillColor ="white",fillOpacity = 1)
mapStates
us = map_data("county")
leaflet(data = us) %>% addPolygons(color = "black",weight = 0.5,fillColor ="white",fillOpacity = 1)
us
mapStates = map("state",plot=FALSE)
knitr::opts_chunk$set(echo = TRUE)
library(stringr)
library(leaflet)
library(maps)
library(RColorBrewer)
library(usmap)
library(tidyverse)
library(tidyverse)
library(drat)
library(hurricaneexposuredata)
library(hurricaneexposure)
rain$state=fips_info(rain$fips)$full
rain$state=fips_info(rain$fips)$full
rain$county=fips_info(rain$fips)$county
## combine location and fips
data(county.fips)
county.fips %>%
as_tibble %>%
separate(polyname, c("region", "subregion"), "," ) -> dfips
map_data("county") %>%
left_join(dfips) -> dall
dall$fips=as.character(dall$fips)
str_split(dall$fips,n=2,pattern="0")
View(dall)
str_split(rain$fips,n=2,pattern="0")
str_split(rain$fips,n=2,pattern="0")[2]
str_split(rain$fips,n=2,pattern="0")[,2]
separate(a,c("lab1","fips"),sep="")
a=str_split(rain$fips,n=2,pattern="0")
separate(a,c("lab1","fips"),sep="")
a=str_split(rain$fips,n=2,pattern="0")
separate(a,c("lab1","fips"),sep="")
a[2]
a[[2]]
a[1]
a
separate(rain$fips,c("lab1","fips"),sep="0")
separate(rain$fips,c("lab1","fips"),sep="0")
separate(rain$fips,c("lab1","fips"),sep="")
a=data.frame(str_split(rain$fips,n=2,pattern="0"))
a=data.frame(str_split(rain$fips,n=2,pattern="0"))
#join by "fips"
newdall = dall%>%
left_join(new
rain %>% separate(`fips`,c("lab","fips"),sep = "0")
rain %>% separate(`fips`,c("lab","fips"),sep = "0")
View(rain)
View(a)
a
separate(a,c("lab1","fips"),sep="")
a=as_tibble(str_split(rain$fips,n=2,pattern="0"))
separate(a,c("lab1","fips"),sep="")
a[855]
a[855][1]
a=str_split_fixed(rain$fips,n=2,pattern="0")
help("str_split")
a
a[,2]
#write.csv(rain,"rain.csv")
newrain=rain%>%separate(county,c("county","lab"))
#write.csv(rain,"rain.csv")
newrain=rain%>%separate(county,c("county","lab"))
rain2=rain
rain2$fips=a[,2]
rain2
#join by "fips"
newdall = dall%>%
left_join(rain2,by=NULL)
newdall
dall
#join by "fips"
newdall = dall%>%
join(rain2,by=NULL)
#join by "fips"
newdall = dall%>%
full_join(rain2,by=NULL)
newdall
newdall
View(dall)
View(dfips)
knitr::opts_chunk$set(warning = FALSE)
library(tidyverse)
library(knitr)
library(stringr)
library(ggmap)
library(magrittr)
library(dplyr)
library(drat)
library(hurricaneexposuredata)
library(hurricaneexposure)
library(magrittr)
library(tmap)
library(tmaptools)
library(maps)
library(usmap)
library(sp)
library(sf)
library(leaflet)
addRepo("geanders")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(corrgram)
library(ggplot2)
library(tidyverse)
library(readxl)
library(pscl)
library(sandwich)
library(lmtest)
library(MASS)
library(magrittr)
mydata=read_excel("FBVapingData_1007.xlsx")
mydata=read_excel("FBVapingData_1007.xlsx")
mydata=data.frame(mydata)
mydata$Page.stance=factor(mydata$Page.stance,labels = c("anti","pro"))
# Select variables used for models, make proportional version of independent variables
vari = dplyr::select(mydata, all_reactions, Post_moral_words, post_CareHarm, post_AuthSub, Page.stance, like, post_wordcount, follow,cmt_anger:cmt_trust,cmt_moral_words,cmt_CareHarm,cmt_AuthSub) %>% mutate(prop_moral = Post_moral_words/post_wordcount, prop_careharm = post_CareHarm/post_wordcount, prop_authsub = post_AuthSub/post_wordcount)
# Formula_0
formula_0 = as.formula("all_reactions ~ Post_moral_words + post_CareHarm + post_AuthSub + post_wordcount+ Page.stance + follow")
# Formula_1
formula_1 = as.formula("all_reactions ~ prop_moral+ prop_careharm + prop_authsub + Page.stance")
install.packages("glmmTMB")
install.packages("mgcv")
help(glmmTMB)
library(glmmTMB)
help("glmmTMB")
fit_glmmTMB <- glmmTMB(all_reactions ~prop_careharm +prop_authsub +(1|Page.stance),data  = vari,family = poisson)
library(mgcv)
fit_glmmTMB <- glmmTMB(all_reactions ~prop_careharm +prop_authsub +(1|Page.stance),data  = vari,family = poisson)
library(glmmTMB)
library(mgcv)
fit_gam = gam(all_reactions ~prop_careharm +prop_authsub +(1|Page.stance),data  = vari,family = nb, method = "ML")
library(glmmTMB)
library(mgcv)
fit_gam = gam(all_reactions ~prop_careharm +prop_authsub + Page.stance,data  = vari,family = nb, method = "ML")
fit_glmmTMB <- glmmTMB(all_reactions ~prop_careharm +prop_authsub +(1|Page.stance),data  = vari,family = nbinom2
fit_glmmTMB <- glmmTMB(all_reactions ~prop_careharm +prop_authsub +(1|Page.stance),data  = vari,family = nbinom2)
library(glmmTMB)
library(mgcv)
fit_gam = gam(all_reactions ~prop_careharm +prop_authsub + Page.stance,data  = vari,family = nb, method = "ML")
fit_glmmTMB <- glmmTMB(all_reactions ~prop_careharm +prop_authsub +(1|Page.stance),data  = vari,family = nbinom2)
install.packages("TMB")
install.packages("TMB")
install.packages("TMB")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(corrgram)
library(ggplot2)
library(tidyverse)
library(readxl)
library(pscl)
library(sandwich)
library(lmtest)
library(MASS)
library(magrittr)
library(tmb)
library(TMB)
fit_glmmTMB <- glmmTMB(all_reactions ~prop_careharm +prop_authsub +(1|Page.stance),data  = vari,family = nbinom2)
library(glmmTMB)
fit_glmmTMB <- glmmTMB(all_reactions ~prop_careharm +prop_authsub +(1|Page.stance),data  = vari,family = nbinom2)
fit_glmmTMB <- glmmTMB(all_reactions ~prop_careharm +prop_authsub +(1|Page.stance),data  = vari,family = nbinom2，REML = TRUE, sparseX=c(cond=TRUE))
fit_glmmTMB <- glmmTMB(all_reactions ~prop_careharm +prop_authsub +(1|Page.stance),data  = vari,family = nbinom2,REML = TRUE, sparseX=c(cond=TRUE))
help("glmmTMB")
glmmTMB(all_reactions ~prop_careharm +prop_authsub +(1|Page.stance),data  = vari,family = nbinom2)
glmmTMB(all_reactions ~prop_careharm +prop_authsub +(1|Page.stance),data  = vari,family = nbinom2)
summary(fit_gam)
summary(fit_TMB)
library(glmmTMB)
library(mgcv)
library(TMB)
fit_gam = gam(all_reactions ~prop_careharm +prop_authsub + Page.stance,data  = vari,family = nb, method = "ML")
fit_TMB=glmmTMB(all_reactions ~prop_careharm +prop_authsub +(1|Page.stance),data  = vari,family = nbinom2)
fit_gam = gam(all_reactions ~prop_careharm +prop_authsub + Page.stance,data  = vari,family = nb, method = "ML")
summary*fit_gam
summary(fit_gam)
summary(glmmTMB(all_reactions ~prop_careharm +prop_authsub +(1|Page.stance),data  = vari,family = nbinom2))
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(corrgram)
library(ggplot2)
library(tidyverse)
library(readxl)
library(pscl)
library(sandwich)
library(lmtest)
library(MASS)
library(magrittr)
library("glmmTMB")
library("mgcv")
theme_set(theme_bw())
library("ggstance")
tmb_coefs
mydata=read_excel("FBVapingData_1007.xlsx")
mydata=data.frame(mydata)
mydata$Page.stance=factor(mydata$Page.stance,labels = c("anti","pro"))
# Select variables used for models, make proportional version of independent variables
vari = dplyr::select(mydata, all_reactions, Post_moral_words, post_CareHarm, post_AuthSub, Page.stance, like, post_wordcount, follow,cmt_anger:cmt_trust,cmt_moral_words,cmt_CareHarm,cmt_AuthSub) %>% mutate(prop_moral = Post_moral_words/post_wordcount, prop_careharm = post_CareHarm/post_wordcount, prop_authsub = post_AuthSub/post_wordcount)
# Formula_0
formula_0 = as.formula("all_reactions ~ Post_moral_words + post_CareHarm + post_AuthSub + post_wordcount+ Page.stance + follow")
# Formula_1
formula_1 = as.formula("all_reactions ~ prop_moral+ prop_careharm + prop_authsub + Page.stance")
nbm2 <- glmmTMB(all_reactions ~ prop_moral+ prop_careharm +
prop_authsub + (1|Page.stance),
data = vari,family = nbinom2)
knitr::opts_chunk$set(echo = TRUE)
set.seed(1)
y <- rnorm(100)
x <- rnorm(100)
y <- x - 2 * x^2 + rnorm(100)
plot(x, y)
set.seed(1)
x <- matrix(rnorm(1000 * 20), 1000, 20)
b <- rnorm(20)
b[3] <- 0
b[4] <- 0
b[9] <- 0
b[19] <- 0
b[10] <- 0
eps <- rnorm(1000)
y <- x %*% b + eps
train <- sample(seq(1000), 100, replace = FALSE)
test <- -train
x.train <- x[train, ]
x.test <- x[test, ]
y.train <- y[train]
y.test <- y[test]
data.train <- data.frame(y = y.train, x = x.train)
regfit.full <- regsubsets(y ~ ., data = data.train, nvmax = 20)
help(regsubsets)
??regsubsets
install.packages("broom")
regfit.full <- broom::regsubsets(y ~ ., data = data.train, nvmax = 20)
install.packages("leaps")
library(leaps)
regfit.full <- regsubsets(y ~ ., data = data.train, nvmax = 20)
library(leaps)
data.train <- data.frame(y = y.train, x = x.train)
regfit.full <- regsubsets(y ~ ., data = data.train, nvmax = 20)
train.mat <- model.matrix(y ~ ., data = data.train, nvmax = 20)
val.errors <- rep(NA, 20)
for (i in 1:20) {
coefi <- coef(regfit.full, id = i)
pred <- train.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((pred - y.train)^2)
}
plot(val.errors, xlab = "Number of predictors", ylab = "Training MSE", pch = 19, type = "b")
data.test <- data.frame(y = y.test, x = x.test)
test.mat <- model.matrix(y ~ ., data = data.test, nvmax = 20)
val.errors <- rep(NA, 20)
for (i in 1:20) {
coefi <- coef(regfit.full, id = i)
pred <- test.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((pred - y.test)^2)
}
plot(val.errors, xlab = "Number of predictors", ylab = "Test MSE", pch = 19, type = "b")
which.min(val.errors)
coef(regfit.full, which.min(val.errors))
set.seed(1)
x <- matrix(rnorm(1000 * 20), 1000, 20)
b <- rnorm(20)
b[3] <- 0
b[4] <- 0
b[9] <- 0
b[19] <- 0
b[10] <- 0
eps <- rnorm(1000)
y <- x %*% b + eps
train <- sample(seq(1000), 100, replace = FALSE)
test <- -train
x.train <- x[train, ]
x.test <- x[test, ]
y.train <- y[train]
y.test <- y[test]
library(leaps)
data.train <- data.frame(y = y.train, x = x.train)
regfit.full <- regsubsets(y ~ ., data = data.train, nvmax = 20)
train.mat <- model.matrix(y ~ ., data = data.train, nvmax = 20)
val.errors <- rep(NA, 20)
for (i in 1:20) {
coefi <- coef(regfit.full, id = i)
pred <- train.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((pred - y.train)^2)
}
plot(val.errors, xlab = "Number of predictors", ylab = "Training MSE", pch = 19, type = "b")
data.test <- data.frame(y = y.test, x = x.test)
test.mat <- model.matrix(y ~ ., data = data.test, nvmax = 20)
val.errors <- rep(NA, 20)
for (i in 1:20) {
coefi <- coef(regfit.full, id = i)
pred <- test.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((pred - y.test)^2)
}
plot(val.errors, xlab = "Number of predictors", ylab = "Test MSE", pch = 19, type = "b")
coef(regfit.full, which.min(val.errors))
val.errors <- rep(NA, 20)
x_cols = colnames(x, do.NULL = FALSE, prefix = "x.")
for (i in 1:20) {
coefi <- coef(regfit.full, id = i)
val.errors[i] <- sqrt(sum((b[x_cols %in% names(coefi)] - coefi[names(coefi) %in% x_cols])^2) + sum(b[!(x_cols %in% names(coefi))])^2)
}
plot(val.errors, xlab = "Number of coefficients", ylab = "Error between estimated and true coefficients", pch = 19, type = "b")
setwd("~/Desktop/MA679/HW")
